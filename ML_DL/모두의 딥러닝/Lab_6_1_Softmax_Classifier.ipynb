{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Lab 6-1 Softmax Classifier.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "byrKuImemb8u"
      },
      "source": [
        "import tensorflow.compat.v1 as tf\n",
        "tf.disable_v2_behavior()\n",
        "\n",
        "x_data=[[1,2,1,1],[2,1,3,2],[3,1,3,4],[4,1,5,5],[1,7,5,5],[1,2,5,6],[1,6,6,6],[1,7,7,7]]\n",
        "y_data=[[0,0,1],[0,0,1],[0,0,1],[0,1,0],[0,1,0],[0,1,0],[1,0,0],[1,0,0]]#one-hot-encoding\n",
        "\n",
        "X=tf.placeholder('float',[None,4])\n",
        "Y=tf.placeholder('float',[None,3])\n",
        "nb_classes=3\n",
        "\n",
        "W=tf.Variable(tf.random_normal([4,nb_classes]),name='weight')\n",
        "b=tf.Variable(tf.random_normal([nb_classes]),name='bias')\n",
        "\n",
        "# Test& one-hot encoding\n",
        "hypothesis=tf.nn.softmax(tf.matmul(X,W)+b)\n",
        "\n",
        "cost=tf.reduce_mean(-tf.reduce_sum(Y*tf.log(hypothesis),axis=1))\n",
        "optimizer=tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost)\n",
        "\n",
        "with tf.Session() as sess:\n",
        "  sess.run(tf.global_variables_initializer())\n",
        "\n",
        "  for step in range(2001):\n",
        "    sess.run(optimizer,feed_dict={X:x_data,Y:y_data})\n",
        "    if step%200==0:\n",
        "      print(step,sess.run(cost,feed_dict={X:x_data,Y:y_data}))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w9RHOeRsnmIE"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}