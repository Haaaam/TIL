{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ml lab10: NN,ReLu,Xavier,Dropout,and Adam.ipynb",
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NRKeD5GFynly"
      },
      "source": [
        "#### Softmax classifier for MNIST"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IpHKkl62kCSn"
      },
      "source": [
        "import tensorflow.compat.v1 as tf\n",
        "tf.disable_v2_behavior()\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "\n",
        "(train_images, train_labels), (test_images, test_labels) = tf.keras.datasets.mnist.load_data()\n",
        "\n",
        "tf.set_random_seed(777)\n",
        "\n",
        "mnist=input_data.read_data_sets(\"MNIST_data/\",one_hot=True)\n",
        "\n",
        "X=tf.placeholder(tf.float32,[None,784])\n",
        "Y=tf.placeholder(tf.loate32,[None,10])\n",
        "\n",
        "W=tf.Variable(tf.random_normal([784,10]))\n",
        "b=tf.Variable(tf.random_normal([10]))\n",
        "hypothesis=tf.matmul(X,W)+b\n",
        "\n",
        "cost=tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=hypothesis,labels=Y))\n",
        "optimizer=tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
        "\n",
        "sess=tf.Session()\n",
        "sess.run(tf.global_variables_initializer())\n",
        "\n",
        "for epoch in range(training_epochs):\n",
        "  avg_cost=0\n",
        "  total_batch=int(mnist.train.num_examples/batch_size)\n",
        "\n",
        "  for i in range(total_batch):\n",
        "    batch_xs,batch_ys=mnist.train.next_batch(batch_size)\n",
        "    feed_dict={X:batch_xs,Y:batch_ys}\n",
        "    c,_=sess.run([cost,optimizer],feed_dict=feed_dict)\n",
        "    avg_cost+=c/total_batch\n",
        "  print('Epoch:','%04d'%(epoch+1),'cost =','{:.9f}'.format(avg_cost))\n",
        "\n",
        "print('Learning Finished!')\n",
        "correct_prediction=tf.equal(tf.argmax(tf.argmax(hypothesis,1),tf.argmax(Y,1)))\n",
        "accuracy=tf.reduce_mean(tf.cast(correct_prediction,tf.float32))\n",
        "print('Accuracy:',sess.run(accuracy,feed_dict={X:mnist.test.images,Y:mnist.test.labels}))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hBrC9Jo42VMn"
      },
      "source": [
        "#### NN for MNIST"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "08AHQRha0kUP"
      },
      "source": [
        "X=tf.placeholder(tf.float32,[None,784])\n",
        "Y=tf.placeholder(tf.float32,[None,10])\n",
        "\n",
        "W1=tf.Variable(tf.random_normal([784,256]))\n",
        "b1=tf.Variable(tf.random_normal([256]))\n",
        "L1=tf.nn.relu(tf.matmul(X,W1)+b1)\n",
        "\n",
        "W2=tf.Variable(tf.random_normal([256,256]))\n",
        "b2=tf.Variable(tf.random_normal([256]))\n",
        "L2=tf.nn.relu(tf.matmul(L1,W2)+b2)\n",
        "\n",
        "W3=tf.Variable(tf.random_normal([256,10]))\n",
        "b3=tf.Variable(tf.random_normal([10]))\n",
        "hypothesis=tf.matmul(L2,W3)+b3\n",
        "\n",
        "#define cost/loss & optimizer\n",
        "cost=tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=hypothesis,labels=Y))\n",
        "optimizer=tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hQ9e2m8S4O8f"
      },
      "source": [
        "#### Xavier for MNIST\n",
        "- xavier initialization tensorflow 검색하면 xavier 실행관련 코드 나옵니다.\n",
        "- 소스코드: W = tf.get_variable(\"W\", shape=[784, 256], initializer=tf.contrib.layers.xavier_initializer())\n",
        " "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zKU8Fivj4Tcb"
      },
      "source": [
        "X=tf.Variable(tf.float32,[None,784])\n",
        "Y=tf.Variable(tf.float32,[None,10])\n",
        "\n",
        "W1=tf.get_variable(\"W1\",shape=[784,256],initializer=tf.contrib.layers.xavier_initializer())\n",
        "b1=tf.Variable(tf.random_normal([256]))\n",
        "L1=tf.nn.relu(tf.matmul(X,W1)+b1)\n",
        "\n",
        "W2=tf.get_variable(\"W2\",shape=[256,256],initializer=tf.contrib.layers.xavier_initializer())\n",
        "b2=tf.Variable(tf.random_normal([256]))\n",
        "L2=tf.nn.relu(tf.matmul(L1,W2)+b2)\n",
        "\n",
        "W3=tf.get_variable(\"W3\",shape=[256,10],initializer=tf.contrib.layers.xavier_initializer())\n",
        "b3=tf.Varible(tf.random_normal([10]))\n",
        "hypothesis=tf.matmul(L2,W3)+b3\n",
        "\n",
        "cost=tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=hypothesis,labels=Y))\n",
        "optimizer=tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iZI72ap667mS"
      },
      "source": [
        "#### Dropout for MNIST"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aCODIHsp5qQS"
      },
      "source": [
        "#dropout(keep_prob)rate 0.7 on training, but should be 1 for testing\n",
        "keep_prop=tf.placeholder(tf.float32)\n",
        "\n",
        "W1=tf.get_variable(\"W1\",shape=[784,512])\n",
        "b1=tf.Variable(tf.random_normal([512]))\n",
        "L1=tf.nn.relu(tf.matmul(X,W1)+b1)\n",
        "L1=tf.nn.dropout(L1,keep_prob=keep_prob)\n",
        "\n",
        "W2=tf.get_variable('W2',shape=[512,512])\n",
        "b2=tf.Variable(tf.random_normal([512]))\n",
        "L2=tf.nn.relu(tf.matmul(L1,W2)+b2)\n",
        "L2=tf.nn.dropout(L2,keep_prob=keep_prob)\n",
        "\n",
        "sess=tf.Session()\n",
        "sess.run(tf.global_variables_initializer())\n",
        "\n",
        "#train my model\n",
        "for epoch in range(training_epochs):\n",
        "  for i in range(total_batch):\n",
        "    batch_xs,batch_ys=mnist.train.next_batch(batch_size)\n",
        "    feed_dict={X:batch_xs,Y:batch_ys,keep_prob:0.7}\n",
        "    c,_=sess.run([cost,optimizer],feed_dict=feed_dict)\n",
        "    avg_cost+=c/total_batch\n",
        "    \n",
        "#Test model and check accuracy\n",
        "correct_prediction=tf.equal(tf.argmax(hypothesis,1),tf.argmax(Y,1))\n",
        "accuracy=tf.reduct_mean(tf.cast(correct_prediction,tf.float32))\n",
        "print('Accuracy:',sess.run(accuracy,feed_dict={X:mnist.test.images,Y:mnist.test.labels,keep_prob:1}))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XpbYRXZe-o58"
      },
      "source": [
        "# Use Adam Optimizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KqDyl2lr9984"
      },
      "source": [
        "cost=tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=hypothesis,labels=Y))\n",
        "optimizer=tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NAEpta32_GtP"
      },
      "source": [
        "#### Summary\n",
        "- Softmax VS Neural Nets for MNIST, 90% and 94.5%\n",
        "- Xavier initialization:97.8%\n",
        "- Deep Neural Nets with Dropout: 98% \n",
        "- Adam and other optimizers\n",
        "- Exercise: Batch Normalization"
      ]
    }
  ]
}