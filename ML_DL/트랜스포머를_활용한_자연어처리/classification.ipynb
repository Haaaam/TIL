{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#### 토큰화"
      ],
      "metadata": {
        "id": "IVT49zoS_XMF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text=\"Tokenizing text is a core task of NLP\"\n",
        "tokenized_text=list(text)\n",
        "print(tokenized_text)"
      ],
      "metadata": {
        "id": "h3XYUQzH6BT9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 각 토큰 수치화\n",
        "token2idx={ch: idx for idx, ch in enumerate(sorted(set(tokenized_text)))}\n",
        "print(token2idx)"
      ],
      "metadata": {
        "id": "YGOPk-bQ_OsZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 토큰화된 텍스트를 정수 리스트로 변환\n",
        "input_ids=[token2idx[token] for token in tokenized_text]\n",
        "print(input_ids)"
      ],
      "metadata": {
        "id": "fhX3xRQM_kM5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "input_ids=torch.tensor(input_ids)\n",
        "one_hot_encodings=F.one_hot(input_ids,num_classes=len(token2idx))\n",
        "one_hot_encodings.shape"
      ],
      "metadata": {
        "id": "7ZoNZCwcB3tv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"토큰: {tokenized_text[0]}\")\n",
        "print(f\"텐서 인덱스: {input_ids[0]}\")\n",
        "print(f\"원-핫 인코딩: {one_hot_encodings[0]}\")"
      ],
      "metadata": {
        "id": "fgKvHZN-C2bJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 단어 토큰화"
      ],
      "metadata": {
        "id": "pQMj_27V-XfV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_text=text.split()\n",
        "print(tokenized_text)"
      ],
      "metadata": {
        "id": "1ZTPI98mDH7D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "model_ckpt=\"distilbert-base-uncased\"\n",
        "tokenizer=AutoTokenizer.from_pretrained(model_ckpt)"
      ],
      "metadata": {
        "id": "VLReRfby-bqX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import DistilBertTokenizer\n",
        "\n",
        "distilbert_tokenizer=DistilBertTokenizer.from_pretrained(model_ckpt)"
      ],
      "metadata": {
        "id": "8PzikaIVDLuZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoded_text=tokenizer(text)\n",
        "print(encoded_text)"
      ],
      "metadata": {
        "id": "oPICdWTtDaMq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 토크나이저의 convert_ids_to_convert() 메서드 사용해서 다시 토큰으로 변환\n",
        "tokens=tokenizer.convert_ids_to_tokens(encoded_text.input_ids)\n",
        "print(tokens)"
      ],
      "metadata": {
        "id": "m39pp9P5DnBQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(tokenizer.convert_tokens_to_string(tokens))"
      ],
      "metadata": {
        "id": "MyTX_xfbOBiy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.vocab_size"
      ],
      "metadata": {
        "id": "yhsVpPJhOfdf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.model_max_length"
      ],
      "metadata": {
        "id": "VUoCL6yyOwMN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.model_input_names"
      ],
      "metadata": {
        "id": "U6W8yU4SO1un"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}