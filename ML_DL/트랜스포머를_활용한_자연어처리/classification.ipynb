{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#### 토큰화"
      ],
      "metadata": {
        "id": "IVT49zoS_XMF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text=\"Tokenizing text is a core task of NLP\"\n",
        "tokenized_text=list(text)\n",
        "print(tokenized_text)"
      ],
      "metadata": {
        "id": "h3XYUQzH6BT9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 각 토큰 수치화\n",
        "token2idx={ch: idx for idx, ch in enumerate(sorted(set(tokenized_text)))}\n",
        "print(token2idx)"
      ],
      "metadata": {
        "id": "YGOPk-bQ_OsZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 토큰화된 텍스트를 정수 리스트로 변환\n",
        "input_ids=[token2idx[token] for token in tokenized_text]\n",
        "print(input_ids)"
      ],
      "metadata": {
        "id": "fhX3xRQM_kM5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "input_ids=torch.tensor(input_ids)\n",
        "one_hot_encodings=F.one_hot(input_ids,num_classes=len(token2idx))\n",
        "one_hot_encodings.shape"
      ],
      "metadata": {
        "id": "7ZoNZCwcB3tv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"토큰: {tokenized_text[0]}\")\n",
        "print(f\"텐서 인덱스: {input_ids[0]}\")\n",
        "print(f\"원-핫 인코딩: {one_hot_encodings[0]}\")"
      ],
      "metadata": {
        "id": "fgKvHZN-C2bJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 단어 토큰화"
      ],
      "metadata": {
        "id": "pQMj_27V-XfV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_text=text.split()\n",
        "print(tokenized_text)"
      ],
      "metadata": {
        "id": "1ZTPI98mDH7D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "model_ckpt=\"distilbert-base-uncased\"\n",
        "tokenizer=AutoTokenizer.from_pretrained(model_ckpt)"
      ],
      "metadata": {
        "id": "VLReRfby-bqX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import DistilBertTokenizer\n",
        "\n",
        "distilbert_tokenizer=DistilBertTokenizer.from_pretrained(model_ckpt)"
      ],
      "metadata": {
        "id": "8PzikaIVDLuZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoded_text=tokenizer(text)\n",
        "print(encoded_text)"
      ],
      "metadata": {
        "id": "oPICdWTtDaMq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 토크나이저의 convert_ids_to_convert() 메서드 사용해서 다시 토큰으로 변환\n",
        "tokens=tokenizer.convert_ids_to_tokens(encoded_text.input_ids)\n",
        "print(tokens)"
      ],
      "metadata": {
        "id": "m39pp9P5DnBQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(tokenizer.convert_tokens_to_string(tokens))"
      ],
      "metadata": {
        "id": "MyTX_xfbOBiy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.vocab_size"
      ],
      "metadata": {
        "id": "yhsVpPJhOfdf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.model_max_length"
      ],
      "metadata": {
        "id": "VUoCL6yyOwMN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.model_input_names"
      ],
      "metadata": {
        "id": "U6W8yU4SO1un"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 트랜스포머를 특성 추출기로 사용하기"
      ],
      "metadata": {
        "id": "ASsd-kJBZcYQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 사전 훈련된 모델 사용\n",
        "from transformers import AutoModel\n",
        "model_ckpt=\"distilbert-base-uncased\"\n",
        "device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model=AutoModel.from_pretrained(model_ckpt).to(device)"
      ],
      "metadata": {
        "id": "EJ961jfQZZV2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 마지막 은닉 상태 추출하기\n",
        "text=\"this is a test\"\n",
        "inputs=tokenizer(text,return_tensors=\"pt\") # 문자열 인코딩\n",
        "print(f\"입력 텐서 크기: {inputs['input_ids'].size()}\")"
      ],
      "metadata": {
        "id": "dLcaTNI1Z05N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inputs={k:v.to(device) for k,v in inputs.items()}\n",
        "with torch.no_grad():\n",
        "  outputs=model(**inputs)\n",
        "print(outputs)"
      ],
      "metadata": {
        "id": "TXq_rjp6aSIN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# last hidden state 크기 확인\n",
        "outputs.last_hidden_state.size() # batch_size x n_tokens x hidden_dim"
      ],
      "metadata": {
        "id": "yVFRNCEhak98"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "outputs.last_hidden_state[:,0].size()"
      ],
      "metadata": {
        "id": "O3NFTEYaax0d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_hidden_states(batch):\n",
        "  # 모델 입력을 GPU로 옮깁니다.\n",
        "  inputs={k:v.to(device) for k,v in batch.items()\n",
        "          if k in tokenizer.model_input_names}\n",
        "  # 마지막 은닉 상태를 추출\n",
        "  with torch.no_grad():\n",
        "    last_hidden_state=model(**inputs).last_hidden_state\n",
        "  # [CLS] 토큰에 대한 벡터를 반환\n",
        "  return {\"hidden_state\": last_hidden_state[:,0].cpu().numpy()}"
      ],
      "metadata": {
        "id": "q_Uq2ctOa_iE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kQNT-mAabhOM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}