{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#### Quantization 종류\n",
        "- 기존의 high precision(일반적으로 fp32) Neural network의 weights와 activation을 더 적은 bit(low precision)으로 변환하는 것\n",
        "- Quantized Matrix Multiplication, Activation, Layer fusion,,,\n",
        "\n",
        "#### ex) quantized code\n",
        "- quantized_cod=uint8(((original-min)/(max-min))*255)\n",
        "\n",
        "- reconstructured=((quantized_code/255.0)*(max-min)+min"
      ],
      "metadata": {
        "id": "7sXMEaYm7y1E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Quantization approach 구분\n",
        "- Post Training Quantization(PTQ): 학습 후에 quantization parameter(scale, shift)를 결정\n",
        "- Quantization Aware Training(QAT):학습 과정에 quantization을 emulate함으로써, quantization으로 발생하는 성능하락을 완화함"
      ],
      "metadata": {
        "id": "H2YcYQeX9Qnr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### PTQ 기법 정리\n",
        "- Dynamic range quantization(weight only quantization): weight만 quantize됨(8bit), inference 시에는 floating-point로 변환되어 수행\n",
        "- Full integer quantization(weight and activation quantization): weight와 더불어 모델의 입력 데이터, activation(중간 레이어의 output)들 또한 quantize, Post Training Quantization, Quantization-Aware Training이 여기에 속함\n",
        "- Float16 quantization: fp32의 데이터 타입의 weight를 fp16으로 quantize\n",
        "\n",
        "참고: Pytorch 구분\n",
        "- Dynamic range quantization(Dynamic quantization)을 별도의 범주로 구분\n",
        "- Post Training Quantization은 Static quantization이라 칭함\n",
        "- Dynamic quantization의 경우, model 수행 시간이 weights를 load하는 것이 실제 matrix multiplication보다 더 오래 걸리는 LSTM, Transformer 기반의 모델에 효과적이라는 언급이 있음"
      ],
      "metadata": {
        "id": "7iLFCJSS9sY8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1) Dynamic range quantization(weight only quantization)\n",
        "- 네트워크의 Weight만 quantize됨(8bit)\n",
        "- Pros:\n",
        "  - 별도의 calibration(validation)데이터가 필요하지 않음\n",
        "  - 모델의 용량 축소(8bit 기준 1/4)\n",
        "- Cons:\n",
        " - 실제 연산은 floating point로 수행됨"
      ],
      "metadata": {
        "id": "T3BdFXMmzEGs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2) Full integer quantization(weight and activation quantization)\n",
        "- Weight와 더불어 모델의 입력 데이터, activation(중간 레이어의 output)들도 quantize됨\n",
        "- Pros:\n",
        "  - 모델의 용량 축소(8bit 기준 1/4)\n",
        "  - 더 적은 메모리 사용량, cache 재사용성 증가\n",
        "  - 빠른 연산(fixed point 8bit 연산을 지원하는 경우)\n",
        "- Cons:\n",
        " - Activation의 parameter를 결정하기 위하여 calibration 데이터가 필요함\n",
        "  (주로 training 데이터에서 사용, 약 100개의 데이터)\n",
        "\n",
        "2) Full integer quantization; TensorRT(NVIDIA) calibration 예시\n",
        "- Calibration?; 성능 저하를 최소로 하는 threshold 찾기\n",
        "- Minimize information loss로 관점으로 접근\n",
        "- 각 네트워크, 각 레이어 마다 activation value의 range, distribution가 다르다(x축: value, y축: normalized histogram counts)\n"
      ],
      "metadata": {
        "id": "8gTI7Ckl_zH-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3) Float16 quantization\n",
        "- Float 32 모델을 float 16모델로 변환\n",
        "- Pros:\n",
        " - 모델의 용량 축소(1/2)\n",
        " - 적은 성능 저하\n",
        " - GPU 상에서 빠른 연산(대체로 fp32를 상회)\n",
        "- Cons:\n",
        " - CPU 상에서는 fixed point 연산만큼의 속도 향상이 있지는 않음"
      ],
      "metadata": {
        "id": "q_bmM4wKFwNV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1.3 Quantization Aware Training(QAT)\n",
        "- 학습 과정에서 quantization을 emulate하여(fake quantization), inference시에 발생하는 quantization error를 training 시점에 반영가능하도록 함\n",
        "- 보통은 일반적인 방법으로 학습을 진행하고, finetuning으로 QAT 적용\n",
        "- 학습 과정에 emulate된 quantization 파라미터를 inference에도 사용\n",
        "- PTQ 대비 성능 하락 폭이 적음\n",
        "\n",
        "- 학습 과정\n",
        " - 학습 과정 중 quantization을 적용하고, 다시 floating point로 변환함(backprop을 계산하기 위함)\n",
        " - In-out에 대한 gradient를 linear로 가정(straing-through estimator)함으로써 네트워크 학습을 수행"
      ],
      "metadata": {
        "id": "XeuXWj9uGlYg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### PyTorch에서 요구하는 준비사항(PTQ, QAT)\n",
        "1. activation이 어디서 quantized되고 de-quantized되는지 구체화해야 한다. (QuantStub, DeQuantStub module을 사용해야 한다.)\n",
        "2. torch.nn.quantized.FloatFunctional을 사용하여 quantization을 위해 특별한 처리가 필요한 텐서 연산을 module로 랩핑. output quantization parameter를 결정하기 위해 특별한 처리가 필요한 add와 cat과 같은 연산이 그 예시.\n",
        "3. Fuse modules: operation/module을 single module로 결합하여 더 높은 정확도와 성능을 얻을 수 있다. 이는 융합할 모듈 리스트를 가져오는 torch.quantization.fuse_modules() API를 사용하여 수행된다. 현재 다음과 같은 fusion을 지원: [Conv, Relu], [Conv,BatchNorm],[Conv,BatchNorm,Relu],[Linear,Relu]\n"
      ],
      "metadata": {
        "id": "Dkm-oy_bwLo9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# QuantStub, DeQuantStub\n",
        "import torch\n",
        "\n",
        "# quantize 가능한 단순한 floating point Model을 정의\n",
        "class Model(torch.nn.Module):\n",
        "  def __init__(self):\n",
        "    super(Model,self).__init__()\n",
        "\n",
        "    # QuantStub을 이용해 floating point에서 quantized tensor로 변환한다.\n",
        "    self.quant=torch.quantization.QuantStub()\n",
        "\n",
        "    # Conv2d (in channel, out channel, kernel size)\n",
        "    self.conv=torch.nn.Conv2d(3,3,1)\n",
        "    self.relu=torch.nn.ReLU()\n",
        "    self.flatten=torch.nn.Flatten()\n",
        "\n",
        "    # Image size: 32 x 32\n",
        "    self.linear=torch.nn.Linear(3*32*32,10)\n",
        "\n",
        "    # DeQuantStub을 이용해 quantized tensor에서 floating point로 변환한다.\n",
        "    self.dequant=torch.quatnization.DeQuantStub()\n",
        "\n",
        "  def forward(self,x):\n",
        "    x=self.quant(x)\n",
        "    x=self.conv(x)\n",
        "    x=self.relu(x)\n",
        "\n",
        "    x=self.flatten(x)\n",
        "    x=self.linear(x)\n",
        "    x=self.dequant(x)\n",
        "\n",
        "    return x"
      ],
      "metadata": {
        "id": "0ha0o7y1-UvZ"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EKc6IOGg7qNN",
        "outputId": "2611cb27-0d75-465a-edc4-975c1f5ff8f8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=MobileNet_V2_QuantizedWeights.IMAGENET1K_QNNPACK_V1`. You can also use `weights=MobileNet_V2_QuantizedWeights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/ao/quantization/utils.py:339: UserWarning: must run observer before calling calculate_qparams. Returning default values.\n",
            "  warnings.warn(\n",
            "Downloading: \"https://download.pytorch.org/models/quantized/mobilenet_v2_qnnpack_37f702c5.pth\" to /root/.cache/torch/hub/checkpoints/mobilenet_v2_qnnpack_37f702c5.pth\n",
            "100%|██████████| 3.42M/3.42M [00:00<00:00, 50.1MB/s]\n",
            "/usr/local/lib/python3.10/dist-packages/torch/_utils.py:383: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
            "  device=storage.device,\n"
          ]
        }
      ],
      "source": [
        "import torchvision\n",
        "model_quantized=torchvision.models.quantization.mobilenet_v2(pretrained=True,quantize=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model=torchvision.models.mobilenet_v2(pretrained=True)\n",
        "\n",
        "import os\n",
        "import torch\n",
        "\n",
        "def print_model_size(mdl):\n",
        "  torch.save(mdl.state_dict(),\"tmp.pt\")\n",
        "  print(\"%.2f MB\" %(os.path.getsize(\"tmp.pt\")/1e6))\n",
        "  os.remove('tmp.pt')\n",
        "\n",
        "print_model_size(model)\n",
        "print_model_size(model_quantized)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DSpWOMTZ59Dc",
        "outputId": "4636ef6b-cf4c-4a9a-e8d5-c2fa056fae45"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=MobileNet_V2_Weights.IMAGENET1K_V1`. You can also use `weights=MobileNet_V2_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/mobilenet_v2-b0353104.pth\" to /root/.cache/torch/hub/checkpoints/mobilenet_v2-b0353104.pth\n",
            "100%|██████████| 13.6M/13.6M [00:00<00:00, 60.3MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "14.24 MB\n",
            "3.62 MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 학습 후 동적 양자화(Post Training Dynamic Quantization)\n",
        "\n",
        "model_dynamic_quantized=torch.quantization.quantize_dynamic(model,qconfig_spec={torch.nn.Linear},dtype=torch.qint8)"
      ],
      "metadata": {
        "id": "5wEoBHFW-g3F"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 학습 후 정적 양자화(Post Training Static Quantization)\n",
        "\n",
        "backend=\"qnnpack\"\n",
        "model.qconfig = torch.quantization.get_default_qconfig(backend)\n",
        "torch.backends.quantized.engine = backend\n",
        "model_static_quantized = torch.quantization.prepare(model, inplace=False)\n",
        "model_static_quantized = torch.quantization.convert(model_static_quantized, inplace=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g114h74e-v91",
        "outputId": "a9214b2a-65cb-40ca-88c2-b17a2f79b1e1"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/ao/quantization/observer.py:1263: UserWarning: must run observer before calling calculate_qparams.                                    Returning default scale and zero point \n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.qconfig = torch.quantization.get_default_qat_qconfig(backend)\n",
        "model_qat = torch.quantization.prepare_qat(model, inplace=False)\n",
        "# 양자화를 고려한 학습이 여기서 진행됩니다.\n",
        "model_qat = torch.quantization.convert(model_qat.eval(), inplace=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P0wpjF22_BYM",
        "outputId": "325ff85b-8356-43d5-a71d-0d9156bd2c3f"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/ao/quantization/utils.py:339: UserWarning: must run observer before calling calculate_qparams. Returning default values.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 아래 Quantization Aware Training에서도 이 클래스 사용\n",
        "class CustomModel(torch.nn.Module) :\n",
        "  def __init__(self) :\n",
        "    super(CustomModel,self).__init__()\n",
        "    self.Quantizer = torch.quantization.QuantStub()\n",
        "\n",
        "    self.dequantizer = torch.quantization.DeQuantStub()\n",
        "\n",
        "  def forward(self, x) :\n",
        "    x = self.Quantizer(x)\n",
        "    x = self.conv(x)\n",
        "    x = self.batchnorm(x)\n",
        "    x = self.relu(x)\t\t# conv, batchnorm, relu fuse할 예정\n",
        "    x = self.dequantizer(x)\n",
        "    return x\n",
        "model_float32 = CustomModel()"
      ],
      "metadata": {
        "id": "T4Rn5l1G_oqt"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_float32.eval()\t\t# 평가모드\n",
        "model_float32.qconfig = torch.quantization.get_default_qconfig('fbgemm')\n",
        "#model_float32 = torch.quantization.fuse_modules(model_float32,  [['conv', 'batchnorm', 'relu']])\n",
        "#model_float32_prepared = torch.quantization.prepare(model_float32_fused)\n",
        "\n",
        "#model_float32_prepared(input_float32)\t# 실제 dataset으로 파라미터 교정\n",
        "#model_int8 = torch.quantization.convert(model_float32_prepared)\n",
        "#res = model_int8(input_float32)\t\t# 추론"
      ],
      "metadata": {
        "id": "E0Mb4WhX_-h0"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "krYlgtGx__-M"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}