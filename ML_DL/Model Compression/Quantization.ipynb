{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#### Quantization 종류\n",
        "- 기존의 high precision(일반적으로 fp32) Neural network의 weights와 activation을 더 적은 bit(low precision)으로 변환하는 것\n",
        "- Quantized Matrix Multiplication, Activation, Layer fusion,,,\n",
        "\n",
        "#### ex) quantized code\n",
        "- quantized_cod=uint8(((original-min)/(max-min))*255)\n",
        "\n",
        "- reconstructured=((quantized_code/255.0)*(max-min)+min"
      ],
      "metadata": {
        "id": "7sXMEaYm7y1E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Quantization approach 구분\n",
        "- Post Training Quantization(PTQ): 학습 후에 quantization parameter(scale, shift)를 결정\n",
        "- Quantization Aware Training(QAT):학습 과정에 quantization을 emulate함으로써, quantization으로 발생하는 성능하락을 완화함"
      ],
      "metadata": {
        "id": "H2YcYQeX9Qnr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### PTQ 기법 정리\n",
        "- Dynamic range quantization(weight only quantization): weight만 quantize됨(8bit), inference 시에는 floating-point로 변환되어 수행\n",
        "- Full integer quantization(weight and activation quantization): weight와 더불어 모델의 입력 데이터, activation(중간 레이어의 output)들 또한 quantize, Post Training Quantization, Quantization-Aware Training이 여기에 속함\n",
        "- Float16 quantization: fp32의 데이터 타입의 weight를 fp16으로 quantize\n",
        "\n",
        "참고: Pytorch 구분\n",
        "- Dynamic range quantization(Dynamic quantization)을 별도의 범주로 구분\n",
        "- Post Training Quantization은 Static quantization이라 칭함\n",
        "- Dynamic quantization의 경우, model 수행 시간이 weights를 load하는 것이 실제 matrix multiplication보다 더 오래 걸리는 LSTM, Transformer 기반의 모델에 효과적이라는 언급이 있음"
      ],
      "metadata": {
        "id": "7iLFCJSS9sY8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1) Dynamic range quantization(weight only quantization)\n",
        "- 네트워크의 Weight만 quantize됨(8bit)\n",
        "- Pros:\n",
        "  - 별도의 calibration(validation)데이터가 필요하지 않음\n",
        "  - 모델의 용량 축소(8bit 기준 1/4)\n",
        "- Cons:\n",
        " - 실제 연산은 floating point로 수행됨"
      ],
      "metadata": {
        "id": "T3BdFXMmzEGs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2) Full integer quantization(weight and activation quantization)\n",
        "- Weight와 더불어 모델의 입력 데이터, activation(중간 레이어의 output)들도 quantize됨\n",
        "- Pros:\n",
        "  - 모델의 용량 축소(8bit 기준 1/4)\n",
        "  - 더 적은 메모리 사용량, cache 재사용성 증가\n",
        "  - 빠른 연산(fixed point 8bit 연산을 지원하는 경우)\n",
        "- Cons:\n",
        " - Activation의 parameter를 결정하기 위하여 calibration 데이터가 필요함\n",
        "  (주로 training 데이터에서 사용, 약 100개의 데이터)\n",
        "\n",
        "2) Full integer quantization; TensorRT(NVIDIA) calibration 예시\n",
        "- Calibration?; 성능 저하를 최소로 하는 threshold 찾기\n",
        "- Minimize information loss로 관점으로 접근\n",
        "- 각 네트워크, 각 레이어 마다 activation value의 range, distribution가 다르다(x축: value, y축: normalized histogram counts)\n"
      ],
      "metadata": {
        "id": "8gTI7Ckl_zH-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3) Float16 quantization\n",
        "- Float 32 모델을 float 16모델로 변환\n",
        "- Pros:\n",
        " - 모델의 용량 축소(1/2)\n",
        " - 적은 성능 저하\n",
        " - GPU 상에서 빠른 연산(대체로 fp32를 상회)\n",
        "- Cons:\n",
        " - CPU 상에서는 fixed point 연산만큼의 속도 향상이 있지는 않음"
      ],
      "metadata": {
        "id": "q_bmM4wKFwNV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1.3 Quantization Aware Training(QAT)\n",
        "- 학습 과정에서 quantization을 emulate하여(fake quantization), inference시에 발생하는 quantization error를 training 시점에 반영가능하도록 함\n",
        "- 보통은 일반적인 방법으로 학습을 진행하고, finetuning으로 QAT 적용\n",
        "- 학습 과정에 emulate된 quantization 파라미터를 inference에도 사용\n",
        "- PTQ 대비 성능 하락 폭이 적음\n",
        "\n",
        "- 학습 과정\n",
        " - 학습 과정 중 quantization을 적용하고, 다시 floating point로 변환함(backprop을 계산하기 위함)\n",
        " - In-out에 대한 gradient를 linear로 가정(straing-through estimator)함으로써 네트워크 학습을 수행"
      ],
      "metadata": {
        "id": "XeuXWj9uGlYg"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0ha0o7y1-UvZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EKc6IOGg7qNN"
      },
      "outputs": [],
      "source": []
    }
  ]
}