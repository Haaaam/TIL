{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CH7.RNN(2).ipynb",
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hwEaBuwiDtbe"
      },
      "source": [
        "#### Seq2Seq 모델 구현하기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r3Koeaw4D0db"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "vocab_size=256  #총아스키코드의 개수\n",
        "\n",
        "#입력될 원문과 번역문을 아스키 코드의 배열로 정의 및 파이토치의 텐서로 변환\n",
        "x_=list(map(ord,'hello'))\n",
        "y_=list(map(ord,'hola'))\n",
        "print('hello - > ',x_)\n",
        "print('halo - > ',y_)\n",
        "x=torch.LongTensor(x_)\n",
        "y=torch.LongTensor(y_)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XdWxcwi06tFD"
      },
      "source": [
        "# Seq2Seq 모델 클래스 정의\n",
        "class Seq2Seq(nn.Module):\n",
        "  def __init__(self,vocab_size,hidden_size):\n",
        "    super(Seq2Seq,self).__init__()\n",
        "    self.n_layers=1\n",
        "    self.hidden_size=hidden_size\n",
        "    self.embedding=nn.Embedding(vocab_size,hidden_size)\n",
        "    # 인코더, 디코더를 GRU 객체로 정의\n",
        "    self.encoder=nn.GRU(hidden_size,hidden_size) #인코더\n",
        "    self.decoder=nn.GRU(hidden_size,hidden_size) #디코더\n",
        "    self.project=nn.Linear(hidden_size,vocab_size) # 다음 토큰을 예상해내는 작은 신경망 하나 더 만들기\n",
        "  \n",
        "  #forward 함수 정의\n",
        "  def forward(self,inputs,targets):\n",
        "    # 인코더의 첫번째 은닉벡터 정의\n",
        "    initial_state=self._init_state()\n",
        "    #원문('hello')을 구성하는 문자 임베딩\n",
        "    embedding=self.embedding(inputs).unsqueeze(1)\n",
        "    #원문을 인코더에 입력하여 문맥 벡터에 해당하는 encoder_state생성\n",
        "    #생성한 encoder_state를 디코더의 첫번째 은닉벡터로 지정\n",
        "    encoder_output,encoder_state=self.encoder(embedding,initial_state)\n",
        "    decoder_state=encoder_state\n",
        "    # decoder에서 문장의 시작 토큰을 입력받기 위해 아스키값으로 공백문자를 뜻하는 0으로 설정\n",
        "    decoder_input=torch.LongTensor([0])\n",
        "    outputs=[]\n",
        "\n",
        "    # for loop: 아스키 번호 0을 이용해 번역문 토큰을 예측했으면 이 예측한 토큰을 이용해 또 다음 토큰을 예측\n",
        "    for i in range(targets.size()[0]):\n",
        "      # decoder는 첫 번째 토큰과 인코더의 문맥 벡터를 동시에 입력받음\n",
        "      decoder_input=self.embedding(decoder_input).unsqueeze(1)\n",
        "      decoder_output,decoder_state=self.decoder(decoder_input,decoder_state)\n",
        "\n",
        "      projection=self.project(decoder_output)\n",
        "      outputs.append(projection)\n",
        "      #티처 포싱을 이용한 디코더 입력 갱신\n",
        "      decoder_input=torch.LongTensor([targets[i]])\n",
        "    outputs=torch.stack(outputs).squeeze()\n",
        "    return outputs\n",
        "\n",
        "  def _init_state(self,batch_size=1):\n",
        "    weight=next(self.parameters()).data\n",
        "    return weight.new(self.n_layers,batch_size,self.hidden_size).zero_()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PlYiL-TksBQK"
      },
      "source": [
        "# 오차함수와 최적화 알고리즘 정의\n",
        "seq2seq=Seq2Seq(vocab_size,16)\n",
        "criterion=nn.CrossEntropyLoss()\n",
        "optimizer=torch.optim.Adam(seq2seq.parameters(),lr=1e-3)\n",
        "\n",
        "log=[]\n",
        "for i in range(1000):\n",
        "  prediction=seq2seq(x,y)\n",
        "  loss=criterion(prediction,y)\n",
        "  optimizer.zero_grad()\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "  loss_val=loss.data\n",
        "  log.append(loss_val)\n",
        "  if i%100==0:\n",
        "    print('\\n 반복:%d 오차:%s'%(i,loss_val.item()))\n",
        "    _,top1=prediction.data.topk(1,1)\n",
        "  print([chr(c) for c in top1.squeeze().numpy().tolist()])\n",
        "plt.plot(log)\n",
        "plt.ylabel('cross entropy loss')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}